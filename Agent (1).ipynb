{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJygm2q2Opa6",
        "outputId": "ef7fe778-9497-4fd2-f2e4-3960834ac61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.8-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.79)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting langchain-core>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-1.0.8-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.0/473.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed langchain-1.0.8 langchain-core-1.0.7 langchain-openai-1.0.3 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langgraph langchain langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The API Key should be named \"OPENAI_API_KEY\", while creating the key (https://openrouter.ai/settings/keys)"
      ],
      "metadata": {
        "id": "loRR4XgQl9p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "T_jUERbdOy2I"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OGh_7lBPI9T",
        "outputId": "fc9e20e3-694c-422c-e6bc-e5a467bd8be2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")"
      ],
      "metadata": {
        "id": "umSRm4eSPTF_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class CounterState(TypedDict):\n",
        "    count: int"
      ],
      "metadata": {
        "id": "UKHDJtQXPhKi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def increment(state: CounterState) -> dict:\n",
        "    state[\"count\"] += 1\n",
        "    return state"
      ],
      "metadata": {
        "id": "KGI7eE8tPmxf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(CounterState)\n",
        "\n",
        "builder.add_node(\"increment\", increment)\n",
        "\n",
        "# Define the execution order: START -> increment -> END\n",
        "builder.add_edge(START, \"increment\")\n",
        "builder.add_edge(\"increment\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "_9R37jSsPqRL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state: CounterState = {\"count\": 0}\n",
        "\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Output\n",
        "# {'count': 1}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMCgWsGdPtBK",
        "outputId": "d1291276-f612-4d39-dc55-5235cb35d5fb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'count': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "def should_continue(state: CounterState) -> Literal[\"increment\", END]:\n",
        "\n",
        "    if state[\"count\"] <= 15: # keep looping\n",
        "        print(\"count\", state[\"count\"])\n",
        "        return \"increment\"\n",
        "\n",
        "    return END # stop the graph"
      ],
      "metadata": {
        "id": "GJkyLLbePxIY"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(CounterState)\n",
        "\n",
        "builder.add_node(\"increment\", increment)\n",
        "\n",
        "builder.add_edge(START, \"increment\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"increment\",\n",
        "    should_continue,\n",
        "    [\"increment\", END],\n",
        ")\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "result = graph.invoke({\"count\": 0})\n",
        "\n",
        "print(result)\n",
        "\n",
        "# Output\n",
        "# {'count': 3}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfwAmGYoP1Se",
        "outputId": "9f822d0b-a174-412e-9c58-b7939b054288"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count 1\n",
            "count 2\n",
            "count 3\n",
            "count 4\n",
            "count 5\n",
            "count 6\n",
            "count 7\n",
            "count 8\n",
            "count 9\n",
            "count 10\n",
            "count 11\n",
            "count 12\n",
            "count 13\n",
            "count 14\n",
            "count 15\n",
            "{'count': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    user_input: str\n",
        "    response: str"
      ],
      "metadata": {
        "id": "NaZ1IA8PROO4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        ")"
      ],
      "metadata": {
        "id": "tREjN5UrRPaA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "def llm_node(state: AgentState) -> dict:\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "        HumanMessage(content=state[\"user_input\"]),\n",
        "    ]\n",
        "\n",
        "    reply = llm.invoke(messages)\n",
        "\n",
        "    return {\"response\": reply.content}"
      ],
      "metadata": {
        "id": "YST5401VRRs-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"llm\", llm_node)\n",
        "\n",
        "# define the flow: START -> llm -> END\n",
        "builder.add_edge(START, \"llm\")\n",
        "builder.add_edge(\"llm\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "ALUkfCOiRVn6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state: AgentState = {\n",
        "    \"user_input\": \"Tell me about Mason Anderson from Creighton University\",\n",
        "    \"response\": \"\",\n",
        "}\n",
        "\n",
        "result = graph.invoke(initial_state)\n",
        "\n",
        "print(\"User:\", initial_state[\"user_input\"])\n",
        "print(\"Assistant:\", result[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GykLlHBdRiWK",
        "outputId": "b7587a9a-da16-476a-922e-79981722492f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Tell me about Mason Anderson from Creighton University\n",
            "Assistant: As of my last update in October 2023, I do not have specific information about an individual named Mason Anderson from Creighton University. It's possible that he is a student, faculty member, or staff, but without further context or information, I can't provide any details.\n",
            "\n",
            "If you have more specific details about Mason Anderson or the context in which you are inquiring (such as his role, achievements, or area of study), I might be able to help with more general information related to that context. Alternatively, you may want to check Creighton University's official website or contact their administration for the most recent and relevant information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state2: AgentState = {\n",
        "    \"user_input\": \"Can you summarise that in two lines?\",\n",
        "    \"response\": \"\",\n",
        "}\n",
        "result2 = graph.invoke(state2)\n",
        "\n",
        "print(\"\\nTurn 2 - User:\", state2[\"user_input\"])\n",
        "print(\"Turn 2 - Assistant:\", result2[\"response\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvy6H7fyY5-X",
        "outputId": "1d03dc3d-f8cd-4ca9-da0e-923f0d0bb17f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Turn 2 - User: Can you summarise that in two lines?\n",
            "Turn 2 - Assistant: Of course! Please provide the text you'd like me to summarize.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "from operator import add\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "class State(TypedDict):\n",
        "    foo: str\n",
        "    bar: Annotated[list[str], add]\n",
        "\n",
        "def node_a(state: State):\n",
        "    # overwrite foo, append \"a\" to bar\n",
        "    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n",
        "\n",
        "def node_b(state: State):\n",
        "    # overwrite foo, append \"b\" to bar\n",
        "    return {\"foo\": \"b\", \"bar\": [\"b\"]}"
      ],
      "metadata": {
        "id": "8thKkKNza46n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_node(\"node_a\", node_a)\n",
        "builder.add_node(\"node_b\", node_b)\n",
        "\n",
        "builder.add_edge(START, \"node_a\")\n",
        "builder.add_edge(\"node_a\", \"node_b\")\n",
        "builder.add_edge(\"node_b\", END)\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "S5gy2b0lbLfa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "final_state = graph.invoke({\"foo\": \"\", \"bar\": []}, config=config)\n",
        "print(\"Final state:\", final_state)\n",
        "\n",
        "# Output\n",
        "# Final state: {'foo': 'b', 'bar': ['a', 'b']}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmLXOoLTbMf5",
        "outputId": "f4ff7e97-b8f7-4d51-b98b-a2900a5506dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final state: {'foo': 'b', 'bar': ['a', 'b']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = list(graph.get_state_history(config))\n",
        "\n",
        "for i, snap in enumerate(history[::-1]):\n",
        "    print(f\"\\nCheckpoint {i}:\")\n",
        "    print(\"  created_at:\", snap.created_at)\n",
        "    print(\"  node:\", snap.metadata)\n",
        "    print(\"  values:\", snap.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELtyVBjxbQZY",
        "outputId": "8f701aba-d7dc-4f04-aaf8-fffab0e8845b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint 0:\n",
            "  created_at: 2025-11-20T21:51:55.053682+00:00\n",
            "  node: {'source': 'input', 'step': -1, 'parents': {}}\n",
            "  values: {'bar': []}\n",
            "\n",
            "Checkpoint 1:\n",
            "  created_at: 2025-11-20T21:51:55.056191+00:00\n",
            "  node: {'source': 'loop', 'step': 0, 'parents': {}}\n",
            "  values: {'foo': '', 'bar': []}\n",
            "\n",
            "Checkpoint 2:\n",
            "  created_at: 2025-11-20T21:51:55.058063+00:00\n",
            "  node: {'source': 'loop', 'step': 1, 'parents': {}}\n",
            "  values: {'foo': 'a', 'bar': ['a']}\n",
            "\n",
            "Checkpoint 3:\n",
            "  created_at: 2025-11-20T21:51:55.059968+00:00\n",
            "  node: {'source': 'loop', 'step': 2, 'parents': {}}\n",
            "  values: {'foo': 'b', 'bar': ['a', 'b']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "from langchain_core.messages import AnyMessage\n",
        "import operator\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]"
      ],
      "metadata": {
        "id": "HPRqRWNBbWuU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "def chat_llm_node(state: MessagesState) -> dict:\n",
        "\n",
        "    # build prompt from system + existing conversation\n",
        "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
        "    history.extend(state[\"messages\"])\n",
        "\n",
        "    reply = llm.invoke(history)\n",
        "\n",
        "    return {\"messages\": [reply]}"
      ],
      "metadata": {
        "id": "3QsB8Jb3bbvj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"chat_llm\", chat_llm_node)\n",
        "builder.add_edge(START, \"chat_llm\")\n",
        "builder.add_edge(\"chat_llm\", END)\n",
        "\n",
        "graph = builder.compile(checkpointer=checkpointer)"
      ],
      "metadata": {
        "id": "QPLmmTf_bdUh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n"
      ],
      "metadata": {
        "id": "5wf5Dh1AbgX-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n",
        "\n",
        "# Turn 1\n",
        "state1 = {\"messages\": [HumanMessage(content=\"What is GIL in python?\")]}\n",
        "result1 = graph.invoke(state1, config=config)\n",
        "\n",
        "for m in result1[\"messages\"]:\n",
        "    print(type(m).__name__, \":\", m.content)\n",
        "\n",
        "# Turn 2\n",
        "state2 = {\"messages\": [HumanMessage(content=\"Summarise it in two lines\")],}\n",
        "result2 = graph.invoke(state2, config=config)\n",
        "\n",
        "for m in result2[\"messages\"][-2:]:\n",
        "    print(type(m).__name__, \":\", m.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhgqZdMmbjL1",
        "outputId": "32500e99-b232-4899-bb94-c39b91b4caf1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage : What is GIL in python?\n",
            "AIMessage : GIL stands for Global Interpreter Lock. It is a mutex (mutual exclusion lock) that protects access to Python objects, preventing multiple native threads from executing Python bytecode at once. This is particularly important in Python, where the memory management of Python objects is not thread-safe.\n",
            "\n",
            "The main points concerning the GIL are:\n",
            "\n",
            "1. **Single Execution of Bytecode**: Because of the GIL, even if a Python program uses multiple threads, only one thread can execute Python bytecode at a time. This means that multi-threaded Python programs may not effectively utilize multiple CPU cores for CPU-bound tasks.\n",
            "\n",
            "2. **I/O-bound vs CPU-bound**: While the GIL can be a bottleneck for CPU-bound tasks that require a lot of computation, it is less of an issue for I/O-bound tasks, such as network or file I/O. In I/O-bound programs, threads often spend a lot of time waiting for I/O operations to complete, which allows other threads to run.\n",
            "\n",
            "3. **Workarounds**: To bypass GIL limitations for CPU-bound tasks, developers can use multiple processes (via the `multiprocessing` module) instead of threads. Each process has its own Python interpreter and memory space, allowing true parallelism.\n",
            "\n",
            "4. **Impact on Performance**: The GIL can lead to reduced performance in multi-threaded applications, especially those that rely heavily on CPU processing. For certain workloads, developers may choose languages or frameworks that do not have a GIL or that allow for better concurrency.\n",
            "\n",
            "5. **Python Implementations**: The standard implementation of Python, CPython, has a GIL. Other implementations, such as Jython (Python on the Java platform) and IronPython (Python on .NET), do not have a GIL and can offer better multi-threading capabilities.\n",
            "\n",
            "In summary, while the GIL simplifies memory management and makes certain aspects of Python easier to implement and use, it can be a limitation in terms of achieving maximum performance in multi-threaded applications, particularly those that are CPU-bound.\n",
            "HumanMessage : Summarise it in two lines\n",
            "AIMessage : The Global Interpreter Lock (GIL) in Python is a mutex that prevents multiple threads from executing Python bytecode simultaneously, which can limit concurrency in CPU-bound tasks. While it simplifies memory management, it can hinder performance in multi-threaded applications, particularly those requiring heavy computation.\n"
          ]
        }
      ]
    }
  ]
}
